{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvWsXpBXGD7d",
        "outputId": "e4b2ddbc-b8be-4850-cabf-f8bd0abcde42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Wikipedia-API\n",
            "  Downloading wikipedia_api-0.7.1.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from Wikipedia-API) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->Wikipedia-API) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->Wikipedia-API) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->Wikipedia-API) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->Wikipedia-API) (2024.8.30)\n",
            "Building wheels for collected packages: Wikipedia-API\n",
            "  Building wheel for Wikipedia-API (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Wikipedia-API: filename=Wikipedia_API-0.7.1-py3-none-any.whl size=14346 sha256=75d0567ffafc1ca56d8cf27564f6869e9d18a9c23971a125096a02a8efef72f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/96/18/b9201cc3e8b47b02b510460210cfd832ccf10c0c4dd0522962\n",
            "Successfully built Wikipedia-API\n",
            "Installing collected packages: Wikipedia-API\n",
            "Successfully installed Wikipedia-API-0.7.1\n"
          ]
        }
      ],
      "source": [
        "! pip install Wikipedia-API\n",
        "import wikipediaapi\n",
        "import pandas as pd\n",
        "import pandas as pd\n",
        "import wikipediaapi\n",
        "import math\n",
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAwtCcoUHfKf"
      },
      "outputs": [],
      "source": [
        "# load your data into a DataFrame (replace 'data.csv' with your data file name)\n",
        "df = pd.read_csv('/content/drive/MyDrive/COLAB/articleDesc.csv',encoding='iso-8859-1')\n",
        "\n",
        "# randomly sample 10000 rows from the DataFrame\n",
        "sample = df.sample(n=100, replace=False, random_state=42)\n",
        "print(sample.shape)\n",
        "# print the sample\n",
        "sample.to_csv('sample.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "1oMBmkc-7Gfu",
        "outputId": "93648d80-4aa0-430e-8a03-d8abff304b89"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "\"['Unnamed: 0'] not found in axis\"",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-433fbc46f38b>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Drop the 'Unnamed: 0' column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Unnamed: 0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Save the cleaned data to a new CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5579\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5580\u001b[0m         \"\"\"\n\u001b[0;32m-> 5581\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   5582\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5583\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4787\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4788\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4828\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4829\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4830\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4831\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7069\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7070\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask].tolist()} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7071\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7072\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['Unnamed: 0'] not found in axis\""
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the data\n",
        "file_path = '/content/cleaned_sample.csv'  # Replace with the actual file path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Drop the 'Unnamed: 0' column\n",
        "data = data.drop(columns=['Unnamed: 0'])\n",
        "\n",
        "# Save the cleaned data to a new CSV file\n",
        "output_path = 'cleaned_file.csv'  # Replace with your desired output path\n",
        "data.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Cleaned file saved to {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMifXojl76-v",
        "outputId": "4c161e60-f8b3-4bca-9ffd-dc3d73a2727c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-12-e0903059f6d9>:12: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df = df.applymap(lambda x: ' '.join(x.split()) if isinstance(x, str) else x)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned file saved to cleaned_filtered_articles.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = \"/content/cleaned_sample.csv\"  # Update with the correct path to your file\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Replace all occurrences of 'NA', 'NANA', 'NANANANANA', etc., keeping words before and after intact\n",
        "df = df.replace(r'\\bNA\\b|\\bNANA\\b|\\bNANANANANA\\b', '', regex=True)  # Matches entire occurrences\n",
        "df = df.replace(r'NA', '', regex=True)  # Matches embedded occurrences like \"NANAStubNA\"\n",
        "\n",
        "# Remove any residual double spaces or unnecessary whitespace\n",
        "df = df.applymap(lambda x: ' '.join(x.split()) if isinstance(x, str) else x)\n",
        "\n",
        "# Save the cleaned DataFrame back to a new CSV file\n",
        "output_file = \"cleaned_filtered_articles.csv\"\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Cleaned file saved to {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdJKqf8_AFf2",
        "outputId": "a4606ad1-56d6-47a0-f5bb-369481e66d72"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated file saved to final_filtered_articles.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the cleaned CSV file\n",
        "file_path = \"/content/filtered_articles(3).csv\"  # Update with the correct path\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Define the list of class strings to search for\n",
        "class_keywords = ['Stub', 'Start', 'C', 'List', 'FA', 'B', 'GA', 'start', 'WikiProject Southeast Asia']\n",
        "\n",
        "# Create a function to extract the class from a row\n",
        "def extract_class(row):\n",
        "    for keyword in class_keywords:\n",
        "        if any(keyword in str(cell) for cell in row):  # Check if the keyword exists in any column of the row\n",
        "            return keyword\n",
        "    return np.nan  # Return NaN if none of the keywords are found\n",
        "\n",
        "# Apply the function to each row and assign the result to the 'Class' column\n",
        "df['Class'] = df.apply(extract_class, axis=1)\n",
        "\n",
        "# Fill any empty or NaN values in the 'Class' column with 'NaN'\n",
        "df['Class'] = df['Class'].fillna('NaN')\n",
        "\n",
        "# Save the updated DataFrame back to a CSV file\n",
        "output_file = \"final_filtered_articles.csv\"\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Updated file saved to {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFZkJzQ0eKNL",
        "outputId": "4f600d8c-0981-4339-b1de-63bc0ba93571"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final filtered file saved to /content/final_filtered_articles(2).csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/final_filtered_articles.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert 'Article Name' and 'Class' columns to strings to avoid errors with non-string types\n",
        "data['Article Name'] = data['Article Name'].astype(str)\n",
        "data['Class'] = data['Class'].astype(str)\n",
        "\n",
        "# Ensure there are no leading or trailing spaces in the columns\n",
        "data['Article Name'] = data['Article Name'].str.strip()\n",
        "data['Class'] = data['Class'].str.strip()\n",
        "\n",
        "# Remove the 'Class' data from 'Article Name' (if it exists in 'Article Name')\n",
        "data['Article Name'] = data.apply(\n",
        "    lambda row: row['Article Name'].replace(row['Class'], '').strip() if row['Class'] in row['Article Name'] else row['Article Name'],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Save the result to a new CSV file\n",
        "output_path = '/content/final_filtered_articles(2).csv'\n",
        "data.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Final filtered file saved to {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "EgM8jHvJMyvB",
        "outputId": "34b978b2-2c01-4a22-f6b3-cdadb4869527"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-14-9f79ccc04b0f>:13: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  data = data.applymap(lambda x: x.replace('@$@', '') if isinstance(x, str) else x)\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/cleaned_sample.csv'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the uploaded CSV file\n",
        "file_path = '/content/cleaned_filtered_articles.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the dataset to understand its structure\n",
        "data.head(), data.columns\n",
        "# Clean '@$@' from column names\n",
        "data.columns = [col.replace('@$@', '') for col in data.columns]\n",
        "\n",
        "# Clean '@$@' from all cell values in the dataframe\n",
        "data = data.applymap(lambda x: x.replace('@$@', '') if isinstance(x, str) else x)\n",
        "\n",
        "# Display the cleaned data (first few rows and column names)\n",
        "data.head(), data.columns\n",
        "\n",
        "# Rename the column by replacing concatenated words with spaces\n",
        "data.rename(columns={'Article NameVital ArticleLevelClassImportanceTopicWikiproject':\n",
        "                     'Article Name Vital Article Level Class Importance Topic Wikiproject'}, inplace=True)\n",
        "\n",
        "# Display the updated columns\n",
        "data.columns\n",
        "cleaned_file_path = '/content/cleaned_sample.csv'\n",
        "\n",
        "data.to_csv(cleaned_file_path, index=False)\n",
        "\n",
        "\n",
        "\n",
        "cleaned_file_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NZzydeOKGIs",
        "outputId": "6486c299-48b9-4fcf-88ff-13dae5017a59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Article Name@$@Vital Article@$@Level@$@Class@$@Importance@$@Topic@$@Wikiproject\n",
            "0  Population_history_of_ancient_Egypt_Archive_8@...                             \n",
            "1           Sin_Mirar_Atrás@$@NA@$@NA@$@NA@$@NA@$@NA                             \n",
            "2                     RMWFC@$@NA@$@NA@$@NA@$@NA@$@NA                             \n",
            "3  Wilbur_E._Colyer@$@NA@$@NA@$@Stub@$@NA@$@['bio...                             \n",
            "4            Salmson_B.9@$@NA@$@NA@$@Start@$@NA@$@NA                             \n",
            "Index(['Article Name@$@Vital Article@$@Level@$@Class@$@Importance@$@Topic@$@Wikiproject'], dtype='object')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-f7a0b8269c67>:15: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  data = data.applymap(lambda x: x.replace('@$@', '') if isinstance(x, str) else x)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cleaned file saved at: /content/cleaned_sample.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the uploaded CSV file with the specified delimiter\n",
        "file_path = '/content/drive/MyDrive/Datasets/articleDesc.csv'\n",
        "\n",
        "try:\n",
        "    # Specify the delimiter as '@$@'\n",
        "    data = pd.read_csv(file_path, delimiter='@$@', engine='python')\n",
        "\n",
        "    # Display the first few rows and column names to understand the structure\n",
        "    print(data.head())\n",
        "    print(data.columns)\n",
        "\n",
        "    # Clean '@$@' from all cell values in the dataframe (if necessary)\n",
        "    data = data.applymap(lambda x: x.replace('@$@', '') if isinstance(x, str) else x)\n",
        "\n",
        "    # Rename the concatenated column to a readable name (if it exists)\n",
        "    if 'Article NameVital ArticleLevelClassImportanceTopicWikiproject' in data.columns:\n",
        "        data.rename(columns={\n",
        "            'Article NameVital ArticleLevelClassImportanceTopicWikiproject':\n",
        "            'Article Name Vital Article Level Class Importance Topic Wikiproject'\n",
        "        }, inplace=True)\n",
        "\n",
        "    # Save the cleaned data to a new CSV file\n",
        "    cleaned_file_path = '/content/cleaned_sample.csv'\n",
        "    data.to_csv(cleaned_file_path, index=False)\n",
        "\n",
        "    print(f\"Cleaned file saved at: {cleaned_file_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SjPDlKdjqEkK",
        "outputId": "23d32e96-cfd0-453d-8364-7efdcc034bf1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['Article Name, Vital Article Level Class Importance Topic Wikiproject'], dtype='object')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "file_path = '/content/cleaned_sample(4).csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "kvtub2kfKzh7",
        "outputId": "5c462954-eb32-4f61-cf81-1ed2499b38b9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-98d4d382-d164-4b1d-a41c-2e03916d9605\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Article Name Vital Article Level Class Importance Topic Wikiproject</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Population_history_of_ancient_Egypt_Archive_8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sin_Mirar_Atrás</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>RMWFC</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Wilbur_E._ColyerStub['biography', 'military hi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Salmson_B.9Start</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-98d4d382-d164-4b1d-a41c-2e03916d9605')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-98d4d382-d164-4b1d-a41c-2e03916d9605 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-98d4d382-d164-4b1d-a41c-2e03916d9605');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5990d631-e37e-4f39-ab64-281874e1d422\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5990d631-e37e-4f39-ab64-281874e1d422')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5990d631-e37e-4f39-ab64-281874e1d422 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "  Article Name Vital Article Level Class Importance Topic Wikiproject\n",
              "0      Population_history_of_ancient_Egypt_Archive_8                 \n",
              "1                                    Sin_Mirar_Atrás                 \n",
              "2                                              RMWFC                 \n",
              "3  Wilbur_E._ColyerStub['biography', 'military hi...                 \n",
              "4                                   Salmson_B.9Start                 "
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('/content/cleaned_sample.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVE6NU0WWpTJ",
        "outputId": "e16dcadd-2b98-4242-d97c-ab683ab642ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered file saved to /content/filtered_articles.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/cleaned_sample(4).csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# The column name is long and needs to be renamed\n",
        "data.columns = ['Article Name, Vital Article Level Class Importance Topic Wikiproject']\n",
        "\n",
        "# Split the column into \"Article Name\" and \"Class\"\n",
        "# Adjust the delimiter (space or any other delimiter) as per your data structure\n",
        "split_data = data['Article Name, Vital Article Level Class Importance Topic Wikiproject'].str.split(' ', n=1, expand=True)\n",
        "\n",
        "# Rename the columns appropriately\n",
        "split_data.columns = ['Article Name', 'Class']\n",
        "\n",
        "# Extract \"Article Name\" and \"Class\"\n",
        "data['Article Name'] = split_data['Article Name']  # First part is \"Article Name\"\n",
        "data['Class'] = split_data['Class']                # Second part is \"Class\"\n",
        "\n",
        "# Keep only the relevant columns\n",
        "filtered_data = data[['Article Name', 'Class']]\n",
        "\n",
        "# Save the result to a new CSV\n",
        "output_path = '/content/filtered_articles.csv'\n",
        "filtered_data.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Filtered file saved to {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        },
        "id": "6q0bK0SVTqXT",
        "outputId": "a8a08376-6a87-4cc1-f10a-ff7830063d00"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "1",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 1",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-8c7b1de1871e>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Extract \"Article Name\" and \"Class\" (adjust indices as needed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Article Name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# First part is \"Article Name\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Class'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m         \u001b[0;31m# Third part is \"Class\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 1"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/cleaned_sample(4).csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# The column name is long and needs to be renamed\n",
        "data.columns = ['Article Name, Vital Article Level Class Importance Topic Wikiproject']\n",
        "\n",
        "# Split the column into \"Article Name\" and \"Class\"\n",
        "# Adjust the delimiter (space or any other delimiter) as per your data structure\n",
        "split_data = data['Article Name, Vital Article Level Class Importance Topic Wikiproject'].str.split(' ', n=1, expand=True)\n",
        "\n",
        "# Rename the columns appropriately\n",
        "split_data.columns = ['Article Name', 'Class']\n",
        "\n",
        "# Extract \"Article Name\" and \"Class\" (adjust indices as needed)\n",
        "data['Article Name'] = split_data[1]  # First part is \"Article Name\"\n",
        "data['Class'] = split_data[2]         # Third part is \"Class\"\n",
        "\n",
        "# Keep only the relevant columns\n",
        "filtered_data = data[['Article Name', 'Class']]\n",
        "\n",
        "# Save the result to a new CSV\n",
        "output_path = '/content/filtered_articles.csv'\n",
        "filtered_data.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Filtered file saved to {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoPZkGj-XgFs",
        "outputId": "cdbfd8e3-6be8-440f-aaba-8317a89ebd48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Article Name    Population_history_of_ancient_Egypt_Archive_8\n",
            "Class                                                     NaN\n",
            "Name: 0, dtype: object\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('/content/filtered_articles.csv')\n",
        "data.columns\n",
        "#print(data[0])\n",
        "# Accessing by row index (e.g., the first row)\n",
        "print(data.iloc[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tqsNVsgDLAC",
        "outputId": "af4ad29a-7573-409d-a05f-ef925ddfb37f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FA articles: 2390\n",
            "Non-FA articles: 474486\n",
            "FA articles:                                               Article Name  \\\n",
            "549                          Joy_KingstonStub['biography',   \n",
            "1325                              Kimberley_WalshB['banner   \n",
            "1817                            Kim_WestStart['biography',   \n",
            "1983     List_of_football_stadiums_in_the_Faroe_Islands...   \n",
            "2394                                           CanberraYes   \n",
            "...                                                    ...   \n",
            "1047535                        Eva_DanielleStub['florida',   \n",
            "1047628                                Monster_Park['nfl',   \n",
            "1047913                      E._MarinellaStub['companies',   \n",
            "1048135               Johnston_&amp;_MurphyStart['brands',   \n",
            "1048244                           Brooklyn_DeckerC['banner   \n",
            "\n",
            "                                                     Class  \n",
            "549                                             'fashion']  \n",
            "1325     shell', 'biography', 'the x factor', 'yorkshir...  \n",
            "1817                                            'fashion']  \n",
            "1983          'event venues', 'faroe islands', 'football']  \n",
            "2394                4 FA Geography ['australia', 'cities']  \n",
            "...                                                    ...  \n",
            "1047535                  'companies', 'fashion', 'brands']  \n",
            "1047628  'sports facilities', 'san francisco 49ers', 'c...  \n",
            "1047913                                'fashion', 'italy']  \n",
            "1048135  'companies', 'fashion', 'retailing', 'tennessee']  \n",
            "1048244  shell', 'women', 'united states', 'biography',...  \n",
            "\n",
            "[2390 rows x 2 columns]\n",
            "Total rows after balancing: 10000\n",
            "Filtered and balanced file saved to /content/final_filtered_articles(4).csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/final_filtered_articles(2).csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Split the combined column into multiple parts based on the pattern 'NA'\n",
        "#split_data = data['Article Name Vital Article Level Class Importance Topic Wikiproject'].str.split('NA', expand=True)\n",
        "\n",
        "# Extract \"Article Name\" and \"Class\" (adjust indices as needed)\n",
        "#data['Article Name'] = split_data[0]  # First part is \"Article Name\"\n",
        "#data['Class'] = split_data[1]         # Third part is \"Class\"\n",
        "data['Article Name'] = split_data.iloc[:, 0]  # First column (index 0) is \"Article Name\"\n",
        "data['Class'] = split_data.iloc[:, 1]\n",
        "\n",
        "# Keep only the relevant columns\n",
        "filtered_data = data[['Article Name', 'Class']]\n",
        "\n",
        "# Ensure \"Class\" column is not null\n",
        "filtered_data = filtered_data[filtered_data['Class'].notna()]\n",
        "\n",
        "# Separate articles with \"FA\" and those without \"FA\" (case-insensitive)\n",
        "fa_articles = filtered_data[filtered_data['Class'].str.contains('FA', case=False, na=False)]\n",
        "non_fa_articles = filtered_data[~filtered_data['Class'].str.contains('FA', case=False, na=False)]\n",
        "\n",
        "# Check the lengths of the two groups\n",
        "print(f\"FA articles: {len(fa_articles)}\")\n",
        "print(f\"Non-FA articles: {len(non_fa_articles)}\")\n",
        "print(f\"FA articles: {fa_articles}\")\n",
        "\n",
        "# Sample 5000 rows from each group, filling missing rows with duplicates if necessary\n",
        "fa_sample = (\n",
        "    fa_articles.sample(n=5000, random_state=42, replace=len(fa_articles) < 5000)\n",
        "    if len(fa_articles) > 0 else pd.DataFrame(columns=filtered_data.columns)\n",
        ")\n",
        "\n",
        "non_fa_sample = (\n",
        "    non_fa_articles.sample(n=5000, random_state=42, replace=len(non_fa_articles) < 5000)\n",
        "    if len(non_fa_articles) > 0 else pd.DataFrame(columns=filtered_data.columns)\n",
        ")\n",
        "\n",
        "# Combine the samples\n",
        "balanced_sample = pd.concat([fa_sample, non_fa_sample], ignore_index=True)\n",
        "\n",
        "# Verify the resulting dataset size\n",
        "print(f\"Total rows after balancing: {len(balanced_sample)}\")\n",
        "\n",
        "# Save the result to a new CSV\n",
        "output_path = '/content/final_filtered_articles(4).csv'\n",
        "balanced_sample.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Filtered and balanced file saved to {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "R64DP2mopB7h"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/concatenated_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvrroeUKFFyg",
        "outputId": "41de75c5-eb7f-460a-ff68-457ab533dcf0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10963, 8)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        },
        "id": "8cVpfj9ipAcS",
        "outputId": "9642daf8-3e34-4ac1-dd82-3c5e5a000370"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Class\n",
              "Stub     4061\n",
              "Start    2181\n",
              "FA        968\n",
              "C         326\n",
              "List      288\n",
              "B         135\n",
              "GA         33\n",
              "A           2\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Class</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Stub</th>\n",
              "      <td>4061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Start</th>\n",
              "      <td>2181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FA</th>\n",
              "      <td>968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>C</th>\n",
              "      <td>326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>List</th>\n",
              "      <td>288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>B</th>\n",
              "      <td>135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GA</th>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>A</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "class_counts = data['Class'].value_counts()\n",
        "\n",
        "class_counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4H8p0Zwqoq1",
        "outputId": "48218e51-b8a5-4e6b-bb94-3ae6b0253547"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sampled data saved to /content/sample_final_filtered_articles(2).csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = \"/content/final_filtered_articles(2).csv\"  # Update with the correct path\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Define the maximum count for 'FA' and the total sample size\n",
        "fa_max = 963\n",
        "total_sample_size = 10000\n",
        "\n",
        "# Filter rows where the 'Class' column is 'FA' (case-sensitive)\n",
        "fa_data = data[data['Class'] == 'FA'].drop_duplicates()\n",
        "other_data = data[data['Class'] != 'FA']\n",
        "\n",
        "# Ensure FA data does not exceed its available rows\n",
        "fa_sample = fa_data.sample(n=min(fa_max, len(fa_data)), random_state=42)\n",
        "\n",
        "# Calculate remaining sample size\n",
        "remaining_sample_size = total_sample_size - len(fa_sample)\n",
        "\n",
        "# Sample the remaining data\n",
        "other_sample = other_data.sample(n=remaining_sample_size, random_state=42)\n",
        "\n",
        "# Combine sampled data\n",
        "sampled_data = pd.concat([fa_sample, other_sample])\n",
        "\n",
        "# Shuffle the data to ensure randomness and reset the index\n",
        "sampled_data = sampled_data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Save the sampled data to a new CSV file\n",
        "output_path = \"/content/sample_final_filtered_articles(2).csv\"  # Update with your desired path\n",
        "sampled_data.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Sampled data saved to {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNla7cXAQvWM",
        "outputId": "06406b09-7b00-4ebd-c916-538951712442"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['Article Name, Vital Article Level Class Importance Topic Wikiproject'], dtype='object')"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnpvPs7qSTo1",
        "outputId": "f5bf7c44-7d7a-4afb-e1cf-4a23b36fe3e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['Article Name, Vital Article Level Class Importance Topic Wikiproject'], dtype='object')"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnGFjjOqUpwU",
        "outputId": "fa38e9f2-6359-4ca9-f595-401cf2874e03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered data saved to /content/filtered_articles.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/cleaned_sample(4).csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# The column name is long and needs to be renamed\n",
        "data.columns = ['Article Name, Vital Article Level Class Importance Topic Wikiproject']\n",
        "\n",
        "# Split the column into \"Article Name\" and \"Class\"\n",
        "# Adjust the delimiter (space or any other delimiter) as per your data structure\n",
        "split_data = data['Article Name, Vital Article Level Class Importance Topic Wikiproject'].str.split(' ', n=1, expand=True)\n",
        "\n",
        "# Rename the columns appropriately\n",
        "split_data.columns = ['Article Name', 'Class']\n",
        "\n",
        "# Save the result to a new CSV file\n",
        "output_path = '/content/filtered_articles.csv'\n",
        "split_data.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Filtered data saved to {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        },
        "id": "geC728zwrCFc",
        "outputId": "a57bdf1e-8163-471f-bb89-92380dffab6f"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Length mismatch: Expected axis has 76 elements, new values have 6 elements",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-cb214b3791c2>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Assign appropriate column names (adjust based on inspection of split data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msplit_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Article Name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Vital Article Level'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Class'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Importance'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Topic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Wikiproject'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Keep only \"Article Name\" and \"Class\" columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   6311\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6312\u001b[0m             \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6313\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6314\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6315\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mproperties.pyx\u001b[0m in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_set_axis\u001b[0;34m(self, axis, labels)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \"\"\"\n\u001b[1;32m    813\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mset_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAxisInt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;31m# Caller is responsible for ensuring we have an Index object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_set_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    239\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/internals/base.py\u001b[0m in \u001b[0;36m_validate_set_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mnew_len\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mold_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m     99\u001b[0m                 \u001b[0;34mf\"Length mismatch: Expected axis has {old_len} elements, new \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;34mf\"values have {new_len} elements\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Length mismatch: Expected axis has 76 elements, new values have 6 elements"
          ]
        }
      ],
      "source": [
        "# Split the single column into multiple parts based on a delimiter (e.g., space, comma)\n",
        "split_data = data['Article Name, Vital Article Level Class Importance Topic Wikiproject'].str.split(r'\\s+', expand=True)\n",
        "\n",
        "# Assign appropriate column names (adjust based on inspection of split data)\n",
        "split_data.columns = ['Article Name', 'Vital Article Level', 'Class', 'Importance', 'Topic', 'Wikiproject']\n",
        "\n",
        "# Keep only \"Article Name\" and \"Class\" columns\n",
        "filtered_data = split_data[['Article Name', 'Class']]\n",
        "\n",
        "# Save the result to a new CSV file\n",
        "output_path = '/content/filtered_articles.csv'\n",
        "filtered_data.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Filtered data saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9psxghtqLgOm"
      },
      "outputs": [],
      "source": [
        "# Create a Wikipedia object\n",
        "wiki = wikipediaapi.Wikipedia(\n",
        "    'en',\n",
        "    headers={\n",
        "        'User-Agent': 'AditiWalkeDataScienceProject/1.0 (personal project; aditi.walke@example.com)'\n",
        "    }\n",
        ")\n",
        "\n",
        "# Replace `df` with your DataFrame containing the article names\n",
        "df = pd.read_csv('/content/sample_final_filtered_articles(2).csv', encoding='ISO-8859-1')\n",
        "\n",
        "# Load existing data or create a new DataFrame with the correct columns\n",
        "existing_data_file = 'existing_data.csv'\n",
        "if os.path.isfile(existing_data_file):\n",
        "    existing_data = pd.read_csv(existing_data_file, encoding='ISO-8859-1')\n",
        "else:\n",
        "    existing_data = pd.DataFrame(columns=['Title', 'Article Name', 'Summary', 'Full Text', 'Links', 'Sections', 'Categories', 'Class'])\n",
        "\n",
        "# Create a set of existing articles to check for duplicates\n",
        "existing_articles = set(existing_data['Article Name'])\n",
        "\n",
        "# Create a list to store the data\n",
        "data = []\n",
        "\n",
        "# Iterate over the article names and retrieve data from Wikipedia\n",
        "for i, article in enumerate(df['Article Name']):\n",
        "    if article in existing_articles:\n",
        "        continue  # skip if article already exists in existing_data\n",
        "    page = wiki.page(article)\n",
        "    summary = page.summary\n",
        "    full_text = page.text\n",
        "    title = page.title\n",
        "\n",
        "    # Retrieve links and handle JSONDecodeError\n",
        "    try:\n",
        "        links = [link.title for link in page.links.values()]\n",
        "    except json.decoder.JSONDecodeError as e:\n",
        "        links = []\n",
        "\n",
        "    sections = len(page.sections)\n",
        "    categories = [cat[9:] for cat in page.categories.keys()]\n",
        "    article_class = df.loc[i, 'Class']\n",
        "\n",
        "    # Handle encoding errors by replacing them with a question mark\n",
        "    summary = summary.encode('ascii', 'replace').decode('utf-8')\n",
        "    full_text = full_text.encode('ascii', 'replace').decode('utf-8')\n",
        "    title = title.encode('ascii', 'replace').decode('utf-8')\n",
        "    links = [link.encode('ascii', 'replace').decode('utf-8') for link in links]\n",
        "    categories = [cat.encode('ascii', 'replace').decode('utf-8') for cat in categories]\n",
        "\n",
        "    # Add the data to the list\n",
        "    data.append([title, article, summary, full_text, links, sections, categories, article_class])\n",
        "\n",
        "    # Save the data to the existing file every 100 records\n",
        "    if (i + 1) % 100 == 0:\n",
        "        existing_data = pd.concat([existing_data, pd.DataFrame(data, columns=existing_data.columns)], ignore_index=True)\n",
        "        existing_data.to_csv(existing_data_file, index=False, encoding='utf-8')\n",
        "        data = []\n",
        "\n",
        "# Save any remaining data to the file\n",
        "if data:\n",
        "    existing_data = pd.concat([existing_data, pd.DataFrame(data, columns=existing_data.columns)], ignore_index=True)\n",
        "    existing_data.to_csv(existing_data_file, index=False, encoding='utf-8')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "83QXHW9DSvAI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e03a5c3c-51dd-451a-dbc1-01453f3cf771"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  Title                Article Name  \\\n",
            "0    Mario Luigi Ciappi          Mario_Luigi_Ciappi   \n",
            "1     Galactik Football  Rocket_(Galactik_Football)   \n",
            "2     Whitehorse Centre           Whitehorse_Centre   \n",
            "3  Propebela harpularia        Propebela_harpularia   \n",
            "4     Acacia drummondii           Acacia_drummondii   \n",
            "\n",
            "                                             Summary  \\\n",
            "0  Mario Luigi Ciappi, O.P. (6 October 1909 ? 23 ...   \n",
            "1  Galactik Football is an animated television se...   \n",
            "2  Whitehorse Centre is an electoral district whi...   \n",
            "3  Propebela harpularia is a species of sea snail...   \n",
            "4  Acacia drummondii, commonly known as Drummond'...   \n",
            "\n",
            "                                           Full Text  \\\n",
            "0  Mario Luigi Ciappi, O.P. (6 October 1909 ? 23 ...   \n",
            "1  Galactik Football is an animated television se...   \n",
            "2  Whitehorse Centre is an electoral district whi...   \n",
            "3  Propebela harpularia is a species of sea snail...   \n",
            "4  Acacia drummondii, commonly known as Drummond'...   \n",
            "\n",
            "                                               Links  Sections  \\\n",
            "0  ['Aesthetics', 'Alma mater', 'Angelo Verardo',...         7   \n",
            "1  ['101 Dalmatian Street', '2D animation', '3D c...         7   \n",
            "2  ['1992 Yukon general election', '1996 Yukon ge...         3   \n",
            "3  ['Animal', 'Binomial nomenclature', 'Body whor...         4   \n",
            "4  ['Acacia', 'Albany, Western Australia', 'Austr...         4   \n",
            "\n",
            "                                          Categories  Class  \n",
            "0  ['1909 births', '1996 deaths', '20th-century I...  Start  \n",
            "1  ['2000s French animated television series', '2...    NaN  \n",
            "2  ['All articles needing additional references',...  Start  \n",
            "3  [\"Articles with 'species' microformats\", 'Arti...   Stub  \n",
            "4  ['Acacia', 'Acacias of Western Australia', \"Ar...  Start  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV files\n",
        "data_9 = pd.read_csv(\"/content/existing_data(9).csv\")\n",
        "data_12 = pd.read_csv(\"/content/existing_data(12).csv\")\n",
        "\n",
        "# Filter rows from the second file where 'Class' column contains 'FA'\n",
        "filtered_data_12 = data_12[data_12['Class'] == 'FA']\n",
        "\n",
        "# Concatenate the two dataframes\n",
        "concatenated_data = pd.concat([data_9, filtered_data_12], ignore_index=True)\n",
        "\n",
        "# Save to a new CSV file if needed\n",
        "concatenated_data.to_csv(\"/content/concatenated_data.csv\", index=False)\n",
        "\n",
        "# Display the first few rows of the result\n",
        "print(concatenated_data.head())\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}