# -*- coding: utf-8 -*-
"""Wikipedia_Article_Classification_Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lrBGR7nJEjSJcAJhkTZApiTch7Ey9tIG

# **1.Import Libraries**
"""

! pip install Wikipedia-API
import wikipediaapi
import pandas as pd
import wikipediaapi
import math
import json
import os
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split

from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier

from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_auc_score,confusion_matrix

from imblearn.combine import SMOTETomek

import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)

"""# **2.Clean Dataset**"""

dataset=pd.read_csv('/content/drive/MyDrive/Datasets/final_wiki_data.csv')
dataset.head()

print(dataset.shape)

#It's better to work on a copy of the dataset rather than modifying the original dataset.
df = dataset.copy()
df.shape

df.isnull().sum()

df.dropna(subset=['Class'], inplace=True)

df['Class'].value_counts()

"""Although the "Summary" and "Full Text" columns contain null values, I believe we should retain them as they may hold valuable information.

The "Class" column is our dependent variable, and with more than 30% of the data missing, we need to carefully consider how to handle it.
"""

df.duplicated().sum()

df = df.drop_duplicates()

duplicates = df[df.duplicated()]  # Get duplicate rows (based on all columns)
duplicate_count = duplicates.shape[0]  # Count duplicate rows

# Print results
print(f"Number of duplicate rows: {duplicate_count}")
if duplicate_count > 0:
    print("Duplicate rows:")
    print(duplicates)

df.isnull().sum()

df.shape
class_counts = df['Class'].value_counts()
class_counts

df.rename(
    columns=({ 'Full Text': 'Full_Text'}),
    inplace=True,)

df.dtypes

"""Since the "Sections" column is of object type, we need to convert it to an integer type."""

df['Sections'] = pd.to_numeric(df['Sections'], errors='coerce').fillna(0).astype(int)

"""# **3. Encoding**

I attempted to check if the Title and Article name are present in the Full Text, but after adjusting the threshold, I consistently got values of 1 or 0. As they didn't provide valuable insights, I decided to remove these columns.
"""

df.drop(['Title', 'Article Name'], inplace=True, axis=1)

df

"""**2. Since the "Summary" and "Full Text" columns contain text data, we are converting them into the length of the text.**"""

df['Summary_Length'] = df['Summary'].str.len()
df['Full_Text_length'] = df['Full_Text'].str.len()
df.drop(['Full_Text', 'Summary'], inplace=True, axis=1)

"""**3. Since the "Links" column contains a number of links in string format, separated by commas, we will count the links and store the count in a new column.**"""

df['Links_Count'] = df['Links'].str.count(',') + 1
df.drop('Links', inplace=True, axis=1)

"""**3. Since the "Categories" column contains a number of categories in string format, separated by commas, we will count them and store the count in a new column called "Categories_count."**"""

df['Categories_count'] = df['Categories'].str.count(',') + 1
df.drop('Categories', inplace=True, axis=1)
df

"""# **4.EDA**"""

import matplotlib.pyplot as plt
#Get value counts of 'Class' column
class_counts = df['Class'].value_counts()

# calculate percentages
total = sum(class_counts)
percentages = [(count/total)*100 for count in class_counts]

#Plot bar chart
plt.figure(figsize=(8,6))
ax = class_counts.plot(kind='bar')
plt.title('Value counts of Class column')
plt.xlabel('Class')
plt.ylabel('Count')

#Add percentage labels
for i, v in enumerate(class_counts):
  ax.text(i-0.1, v+5, str(round(percentages[i], 2))+'%', color='black', fontsize=10)

plt.show()

df.Class.unique()

df['Class_Binary'] = (df['Class'] == 'FA').astype(int)
df.drop('Class', inplace=True, axis=1)

df.isnull().sum()

df = df.dropna(subset = ['Summary_Length', 'Full_Text_length'])

df

sns.pairplot(df, hue = 'Class_Binary', height=3)

"""# **5.Split dataset**

Separate the dependent and independent features.
"""

X = df.drop(['Class_Binary'], axis=1)
y = df['Class_Binary']

"""Split data into train and test data"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)

"""**Scaling**"""

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""**SMOTE**"""

smote_tomek = SMOTETomek(sampling_strategy=0.75)


X_resampled, y_resampled  = smote_tomek.fit_resample(X, y)

# Checking the class distribution of the resampled data
print(y_resampled .value_counts())

"""# **6.Models**

**1.Logistic Regression Model**
"""

lr_model = LogisticRegression(max_iter=200)
lr_model.fit(X_train, y_train)
test_predicted=lr_model.predict(X_test)
train_predicted=lr_model.predict(X_train)

print()
print()
print ("The accuracy of Logistic Regression on training data : ", accuracy_score(y_train, train_predicted)*100, "%")
print ("The accuracy of Logistic Regression on testing data : ", accuracy_score(y_test, test_predicted)*100, "%")
print ("The aurroc_auc_score of Logistic Regression is : ", roc_auc_score(y_test, lr_model.predict_proba(X_test)[:, 1]))

"""**1. Logistic Regression with SMOTE Sampling Technique**"""

X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled , test_size=0.33, random_state=42)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

lr_model = LogisticRegression(max_iter=100)
lr_model.fit(X_train, y_train)
train_predicted=lr_model.predict(X_train)
test_predicted=lr_model.predict(X_test)
print()
print()

print ("The accuracy of Logistic Regression on training data : ", accuracy_score(y_train, train_predicted)*100, "%")
print ("The accuracy of Logistic Regression on testing data : ", accuracy_score(y_test, test_predicted)*100, "%")
print ("The aurroc_auc_score of Logistic Regression is : ", roc_auc_score(y_test, lr_model.predict_proba(X_test)[:, 1]))

"""**2.Random Forest**"""

# Initialize the Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=0)

# Train the model
rf_model.fit(X_train, y_train)

# Make predictions
predicted_train = rf_model.predict(X_train)  # Predictions for training data
predicted_test = rf_model.predict(X_test)   # Predictions for testing data

# Evaluate and print results
print(f"The accuracy of Random Forest on training dataset: {accuracy_score(y_train, predicted_train) * 100:.2f}%")
print(f"The accuracy of Random Forest on testing dataset: {accuracy_score(y_test, predicted_test) * 100:.2f}%")
print()
print(f"The ROC AUC score of Random Forest is: {roc_auc_score(y_test, rf_model.predict_proba(X_test)[:, 1]):.4f}")

"""**2.1. Random Forest with SMOTE**"""

X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled , test_size=0.33, random_state=42)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

rf_model = RandomForestClassifier(n_estimators = 100, random_state = 0)

rf_model.fit(X_train, y_train)

predicted_train = rf_model.predict(X_train)
predicted_test = rf_model.predict(X_test)


print("The accuracy of Random Forest on training dataset : ", accuracy_score(y_train, predicted_train.round())*100, "%")
print("The accuracy of Random Forest on testing dataset : ", accuracy_score(y_test, predicted_test.round())*100, "%")
print()
print ("The aurroc_auc_score of  random forest is : ", roc_auc_score(y_test, rf_model.predict_proba(X_test)[:, 1]))

"""**3. SVC Model**"""

svm_model = SVC(kernel='rbf', probability=True)

svm_model.fit(X_train, y_train)

predicted_train = svm_model.predict(X_train)
predicted_test = svm_model.predict(X_test)

print("The accuracy of SVM on training dataset : ", accuracy_score(y_train, predicted_train)*100, "%")
print("The accuracy of SVM on testing dataset : ", accuracy_score(y_test, predicted_test)*100, "%")
print()
print("The aucroc score of SVM is : ", roc_auc_score(y_test, svm_model.predict_proba(X_test)[:, 1]))

"""**SVC Model with SMOTE**"""

# Split the dataset into training and testing datasets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.33, random_state=42)

# Scale the features using StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Train a Support Vector Machine classifier
svm_model = SVC(kernel='linear', C=1.0, random_state=42)
svm_model.fit(X_train, y_train)

# Predict the labels of the test dataset
y_pred_test = svm_model.predict(X_test)

# Calculate the accuracy of the classifier
accuracy = accuracy_score(y_test, y_pred_test)
print("The accuracy of SVM on the testing dataset:", accuracy)

# Calculate the AUC score of the classifier
y_prob_test = svm_model.decision_function(X_test)
auc_score = roc_auc_score(y_test, y_prob_test)
print("The AUC score of SVM on the testing dataset:", auc_score)

"""**Oberservation**

Using the SMOTE strategy results in a decrease in accuracy, whereas without SMOTE, the accuracy increases.

# **Post Evalustion**

**1. Whatâ€™s the accuracy you are getting?**

The highest accuracy is achieved with the Random Forest Classifier at 99.38%

followed by Logistic Regression at 99.04%,

and SVM at 93.95%.

**2.What if i choose SVM instead of logistic regression?**

As per accuracy and auroc score SVM performing better than logistic regression

**3. What features do you think are important? Can you get the same accuracy using only a single feature?**

**Feature Importance**
"""

importances = rf_model.feature_importances_
df1 = pd.DataFrame({"Features":pd.DataFrame(X_test).columns,"importances":importances})
df1.set_index("importances")

df1 = df1.sort_values('importances')
df1.plot.bar(color='teal')

df2=df.drop('Class_Binary',axis=1)
feature_names = list(df2.columns)
if 'y' in feature_names:
    feature_names.remove('y')
total_importance = importances.sum()
percent_importances = importances / total_importance * 100

# DataFrame with the feature names and their percent importances
df2 = pd.DataFrame({'feature_names': feature_names, 'percent_importances': percent_importances})

# Sort the DataFrame
df2 = df2.sort_values(by='percent_importances', ascending=False)

#bar plot of the percent importances
plt.bar(x=df2['feature_names'], height=df2['percent_importances'], color='teal')
plt.xticks(rotation=90)
plt.xlabel('Feature')
plt.ylabel('Percent Importance')
plt.show()

"""**When we use the most important feature as the sole independent variable**"""

X_new = X['Full_Text_length'].values.reshape(-1, 1)
y_new= y
X_train, X_test, y_train, y_test = train_test_split(X_new, y_new, test_size=0.33, random_state=42)

# Scale the features using StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

rf_model = RandomForestClassifier(n_estimators = 100, random_state = 0)

rf_model.fit(X_train, y_train)

predicted_train = rf_model.predict(X_train)
predicted_test = rf_model.predict(X_test)

print("The accuracy of Random Forest on training dataset : ", accuracy_score(y_train, predicted_train.round())*100, "%")
print("The accuracy of Random Forest on testing dataset : ", accuracy_score(y_test, predicted_test.round())*100, "%")
print()
print ("The aurroc_auc_score of  random forest is : ", roc_auc_score(y_test, rf_model.predict_proba(X_test)[:, 1]))

"""**We get slightly less accuracy, which indicates that other features are also important for the model's prediction.**"""

