# -*- coding: utf-8 -*-
"""Wikipedia_dataset_creation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18SKApzHFAHQQT5HqWDcOosEOuO1qPp7n
"""

! pip install Wikipedia-API
import wikipediaapi
import pandas as pd
import pandas as pd
import wikipediaapi
import math
import json
import os

# load your data into a DataFrame (replace 'data.csv' with your data file name)
df = pd.read_csv('/content/drive/MyDrive/COLAB/articleDesc.csv',encoding='iso-8859-1')

# randomly sample 10000 rows from the DataFrame
sample = df.sample(n=100, replace=False, random_state=42)
print(sample.shape)
# print the sample
sample.to_csv('sample.csv')

import pandas as pd

# Load the data
file_path = '/content/cleaned_sample.csv'  # Replace with the actual file path
data = pd.read_csv(file_path)

# Drop the 'Unnamed: 0' column
data = data.drop(columns=['Unnamed: 0'])

# Save the cleaned data to a new CSV file
output_path = 'cleaned_file.csv'  # Replace with your desired output path
data.to_csv(output_path, index=False)

print(f"Cleaned file saved to {output_path}")

import pandas as pd

# Load the CSV file
file_path = "/content/cleaned_sample.csv"  # Update with the correct path to your file
df = pd.read_csv(file_path)

# Replace all occurrences of 'NA', 'NANA', 'NANANANANA', etc., keeping words before and after intact
df = df.replace(r'\bNA\b|\bNANA\b|\bNANANANANA\b', '', regex=True)  # Matches entire occurrences
df = df.replace(r'NA', '', regex=True)  # Matches embedded occurrences like "NANAStubNA"

# Remove any residual double spaces or unnecessary whitespace
df = df.applymap(lambda x: ' '.join(x.split()) if isinstance(x, str) else x)

# Save the cleaned DataFrame back to a new CSV file
output_file = "cleaned_filtered_articles.csv"
df.to_csv(output_file, index=False)

print(f"Cleaned file saved to {output_file}")

import pandas as pd
import numpy as np

# Load the cleaned CSV file
file_path = "/content/filtered_articles(3).csv"  # Update with the correct path
df = pd.read_csv(file_path)

# Define the list of class strings to search for
class_keywords = ['Stub', 'Start', 'C', 'List', 'FA', 'B', 'GA', 'start', 'WikiProject Southeast Asia']

# Create a function to extract the class from a row
def extract_class(row):
    for keyword in class_keywords:
        if any(keyword in str(cell) for cell in row):  # Check if the keyword exists in any column of the row
            return keyword
    return np.nan  # Return NaN if none of the keywords are found

# Apply the function to each row and assign the result to the 'Class' column
df['Class'] = df.apply(extract_class, axis=1)

# Fill any empty or NaN values in the 'Class' column with 'NaN'
df['Class'] = df['Class'].fillna('NaN')

# Save the updated DataFrame back to a CSV file
output_file = "final_filtered_articles.csv"
df.to_csv(output_file, index=False)

print(f"Updated file saved to {output_file}")

import pandas as pd

# Load the dataset
file_path = '/content/final_filtered_articles.csv'
data = pd.read_csv(file_path)

# Convert 'Article Name' and 'Class' columns to strings to avoid errors with non-string types
data['Article Name'] = data['Article Name'].astype(str)
data['Class'] = data['Class'].astype(str)

# Ensure there are no leading or trailing spaces in the columns
data['Article Name'] = data['Article Name'].str.strip()
data['Class'] = data['Class'].str.strip()

# Remove the 'Class' data from 'Article Name' (if it exists in 'Article Name')
data['Article Name'] = data.apply(
    lambda row: row['Article Name'].replace(row['Class'], '').strip() if row['Class'] in row['Article Name'] else row['Article Name'],
    axis=1
)

# Save the result to a new CSV file
output_path = '/content/final_filtered_articles(2).csv'
data.to_csv(output_path, index=False)

print(f"Final filtered file saved to {output_path}")

import pandas as pd

# Load the uploaded CSV file
file_path = '/content/cleaned_filtered_articles.csv'
data = pd.read_csv(file_path)

# Display the first few rows of the dataset to understand its structure
data.head(), data.columns
# Clean '@$@' from column names
data.columns = [col.replace('@$@', '') for col in data.columns]

# Clean '@$@' from all cell values in the dataframe
data = data.applymap(lambda x: x.replace('@$@', '') if isinstance(x, str) else x)

# Display the cleaned data (first few rows and column names)
data.head(), data.columns

# Rename the column by replacing concatenated words with spaces
data.rename(columns={'Article NameVital ArticleLevelClassImportanceTopicWikiproject':
                     'Article Name Vital Article Level Class Importance Topic Wikiproject'}, inplace=True)

# Display the updated columns
data.columns
cleaned_file_path = '/content/cleaned_sample.csv'

data.to_csv(cleaned_file_path, index=False)



cleaned_file_path

import pandas as pd

# Load the uploaded CSV file with the specified delimiter
file_path = '/content/drive/MyDrive/Datasets/articleDesc.csv'

try:
    # Specify the delimiter as '@$@'
    data = pd.read_csv(file_path, delimiter='@$@', engine='python')

    # Display the first few rows and column names to understand the structure
    print(data.head())
    print(data.columns)

    # Clean '@$@' from all cell values in the dataframe (if necessary)
    data = data.applymap(lambda x: x.replace('@$@', '') if isinstance(x, str) else x)

    # Rename the concatenated column to a readable name (if it exists)
    if 'Article NameVital ArticleLevelClassImportanceTopicWikiproject' in data.columns:
        data.rename(columns={
            'Article NameVital ArticleLevelClassImportanceTopicWikiproject':
            'Article Name Vital Article Level Class Importance Topic Wikiproject'
        }, inplace=True)

    # Save the cleaned data to a new CSV file
    cleaned_file_path = '/content/cleaned_sample.csv'
    data.to_csv(cleaned_file_path, index=False)

    print(f"Cleaned file saved at: {cleaned_file_path}")
except Exception as e:
    print(f"Error: {e}")

file_path = '/content/cleaned_sample(4).csv'
data = pd.read_csv(file_path)

data.columns

df = pd.read_csv('/content/cleaned_sample.csv')
df.head()

import pandas as pd

# Load the dataset
file_path = '/content/cleaned_sample(4).csv'
data = pd.read_csv(file_path)

# The column name is long and needs to be renamed
data.columns = ['Article Name, Vital Article Level Class Importance Topic Wikiproject']

# Split the column into "Article Name" and "Class"
# Adjust the delimiter (space or any other delimiter) as per your data structure
split_data = data['Article Name, Vital Article Level Class Importance Topic Wikiproject'].str.split(' ', n=1, expand=True)

# Rename the columns appropriately
split_data.columns = ['Article Name', 'Class']

# Extract "Article Name" and "Class"
data['Article Name'] = split_data['Article Name']  # First part is "Article Name"
data['Class'] = split_data['Class']                # Second part is "Class"

# Keep only the relevant columns
filtered_data = data[['Article Name', 'Class']]

# Save the result to a new CSV
output_path = '/content/filtered_articles.csv'
filtered_data.to_csv(output_path, index=False)

print(f"Filtered file saved to {output_path}")

import pandas as pd

# Load the dataset
file_path = '/content/cleaned_sample(4).csv'
data = pd.read_csv(file_path)

# The column name is long and needs to be renamed
data.columns = ['Article Name, Vital Article Level Class Importance Topic Wikiproject']

# Split the column into "Article Name" and "Class"
# Adjust the delimiter (space or any other delimiter) as per your data structure
split_data = data['Article Name, Vital Article Level Class Importance Topic Wikiproject'].str.split(' ', n=1, expand=True)

# Rename the columns appropriately
split_data.columns = ['Article Name', 'Class']

# Extract "Article Name" and "Class" (adjust indices as needed)
data['Article Name'] = split_data[1]  # First part is "Article Name"
data['Class'] = split_data[2]         # Third part is "Class"

# Keep only the relevant columns
filtered_data = data[['Article Name', 'Class']]

# Save the result to a new CSV
output_path = '/content/filtered_articles.csv'
filtered_data.to_csv(output_path, index=False)

print(f"Filtered file saved to {output_path}")

data = pd.read_csv('/content/filtered_articles.csv')
data.columns
#print(data[0])
# Accessing by row index (e.g., the first row)
print(data.iloc[0])

import pandas as pd

# Load the dataset
file_path = '/content/final_filtered_articles(2).csv'
data = pd.read_csv(file_path)

# Split the combined column into multiple parts based on the pattern 'NA'
#split_data = data['Article Name Vital Article Level Class Importance Topic Wikiproject'].str.split('NA', expand=True)

# Extract "Article Name" and "Class" (adjust indices as needed)
#data['Article Name'] = split_data[0]  # First part is "Article Name"
#data['Class'] = split_data[1]         # Third part is "Class"
data['Article Name'] = split_data.iloc[:, 0]  # First column (index 0) is "Article Name"
data['Class'] = split_data.iloc[:, 1]

# Keep only the relevant columns
filtered_data = data[['Article Name', 'Class']]

# Ensure "Class" column is not null
filtered_data = filtered_data[filtered_data['Class'].notna()]

# Separate articles with "FA" and those without "FA" (case-insensitive)
fa_articles = filtered_data[filtered_data['Class'].str.contains('FA', case=False, na=False)]
non_fa_articles = filtered_data[~filtered_data['Class'].str.contains('FA', case=False, na=False)]

# Check the lengths of the two groups
print(f"FA articles: {len(fa_articles)}")
print(f"Non-FA articles: {len(non_fa_articles)}")
print(f"FA articles: {fa_articles}")

# Sample 5000 rows from each group, filling missing rows with duplicates if necessary
fa_sample = (
    fa_articles.sample(n=5000, random_state=42, replace=len(fa_articles) < 5000)
    if len(fa_articles) > 0 else pd.DataFrame(columns=filtered_data.columns)
)

non_fa_sample = (
    non_fa_articles.sample(n=5000, random_state=42, replace=len(non_fa_articles) < 5000)
    if len(non_fa_articles) > 0 else pd.DataFrame(columns=filtered_data.columns)
)

# Combine the samples
balanced_sample = pd.concat([fa_sample, non_fa_sample], ignore_index=True)

# Verify the resulting dataset size
print(f"Total rows after balancing: {len(balanced_sample)}")

# Save the result to a new CSV
output_path = '/content/final_filtered_articles(4).csv'
balanced_sample.to_csv(output_path, index=False)

print(f"Filtered and balanced file saved to {output_path}")

data = pd.read_csv('/content/concatenated_data.csv')

data.shape

class_counts = data['Class'].value_counts()

class_counts

import pandas as pd

# Load the dataset
file_path = "/content/final_filtered_articles(2).csv"  # Update with the correct path
data = pd.read_csv(file_path)

# Define the maximum count for 'FA' and the total sample size
fa_max = 963
total_sample_size = 10000

# Filter rows where the 'Class' column is 'FA' (case-sensitive)
fa_data = data[data['Class'] == 'FA'].drop_duplicates()
other_data = data[data['Class'] != 'FA']

# Ensure FA data does not exceed its available rows
fa_sample = fa_data.sample(n=min(fa_max, len(fa_data)), random_state=42)

# Calculate remaining sample size
remaining_sample_size = total_sample_size - len(fa_sample)

# Sample the remaining data
other_sample = other_data.sample(n=remaining_sample_size, random_state=42)

# Combine sampled data
sampled_data = pd.concat([fa_sample, other_sample])

# Shuffle the data to ensure randomness and reset the index
sampled_data = sampled_data.sample(frac=1, random_state=42).reset_index(drop=True)

# Save the sampled data to a new CSV file
output_path = "/content/sample_final_filtered_articles(2).csv"  # Update with your desired path
sampled_data.to_csv(output_path, index=False)

print(f"Sampled data saved to {output_path}")

data.columns

data.columns

import pandas as pd

# Load the dataset
file_path = '/content/cleaned_sample(4).csv'
data = pd.read_csv(file_path)

# The column name is long and needs to be renamed
data.columns = ['Article Name, Vital Article Level Class Importance Topic Wikiproject']

# Split the column into "Article Name" and "Class"
# Adjust the delimiter (space or any other delimiter) as per your data structure
split_data = data['Article Name, Vital Article Level Class Importance Topic Wikiproject'].str.split(' ', n=1, expand=True)

# Rename the columns appropriately
split_data.columns = ['Article Name', 'Class']

# Save the result to a new CSV file
output_path = '/content/filtered_articles.csv'
split_data.to_csv(output_path, index=False)

print(f"Filtered data saved to {output_path}")

# Split the single column into multiple parts based on a delimiter (e.g., space, comma)
split_data = data['Article Name, Vital Article Level Class Importance Topic Wikiproject'].str.split(r'\s+', expand=True)

# Assign appropriate column names (adjust based on inspection of split data)
split_data.columns = ['Article Name', 'Vital Article Level', 'Class', 'Importance', 'Topic', 'Wikiproject']

# Keep only "Article Name" and "Class" columns
filtered_data = split_data[['Article Name', 'Class']]

# Save the result to a new CSV file
output_path = '/content/filtered_articles.csv'
filtered_data.to_csv(output_path, index=False)

print(f"Filtered data saved to {output_path}")

# Create a Wikipedia object
wiki = wikipediaapi.Wikipedia(
    'en',
    headers={
        'User-Agent': 'AditiWalkeDataScienceProject/1.0 (personal project; aditi.walke@example.com)'
    }
)

# Replace `df` with your DataFrame containing the article names
df = pd.read_csv('/content/sample_final_filtered_articles(2).csv', encoding='ISO-8859-1')

# Load existing data or create a new DataFrame with the correct columns
existing_data_file = 'existing_data.csv'
if os.path.isfile(existing_data_file):
    existing_data = pd.read_csv(existing_data_file, encoding='ISO-8859-1')
else:
    existing_data = pd.DataFrame(columns=['Title', 'Article Name', 'Summary', 'Full Text', 'Links', 'Sections', 'Categories', 'Class'])

# Create a set of existing articles to check for duplicates
existing_articles = set(existing_data['Article Name'])

# Create a list to store the data
data = []

# Iterate over the article names and retrieve data from Wikipedia
for i, article in enumerate(df['Article Name']):
    if article in existing_articles:
        continue  # skip if article already exists in existing_data
    page = wiki.page(article)
    summary = page.summary
    full_text = page.text
    title = page.title

    # Retrieve links and handle JSONDecodeError
    try:
        links = [link.title for link in page.links.values()]
    except json.decoder.JSONDecodeError as e:
        links = []

    sections = len(page.sections)
    categories = [cat[9:] for cat in page.categories.keys()]
    article_class = df.loc[i, 'Class']

    # Handle encoding errors by replacing them with a question mark
    summary = summary.encode('ascii', 'replace').decode('utf-8')
    full_text = full_text.encode('ascii', 'replace').decode('utf-8')
    title = title.encode('ascii', 'replace').decode('utf-8')
    links = [link.encode('ascii', 'replace').decode('utf-8') for link in links]
    categories = [cat.encode('ascii', 'replace').decode('utf-8') for cat in categories]

    # Add the data to the list
    data.append([title, article, summary, full_text, links, sections, categories, article_class])

    # Save the data to the existing file every 100 records
    if (i + 1) % 100 == 0:
        existing_data = pd.concat([existing_data, pd.DataFrame(data, columns=existing_data.columns)], ignore_index=True)
        existing_data.to_csv(existing_data_file, index=False, encoding='utf-8')
        data = []

# Save any remaining data to the file
if data:
    existing_data = pd.concat([existing_data, pd.DataFrame(data, columns=existing_data.columns)], ignore_index=True)
    existing_data.to_csv(existing_data_file, index=False, encoding='utf-8')

import pandas as pd

# Load the CSV files
data_9 = pd.read_csv("/content/existing_data(9).csv")
data_12 = pd.read_csv("/content/existing_data(12).csv")

# Filter rows from the second file where 'Class' column contains 'FA'
filtered_data_12 = data_12[data_12['Class'] == 'FA']

# Concatenate the two dataframes
concatenated_data = pd.concat([data_9, filtered_data_12], ignore_index=True)

# Save to a new CSV file if needed
concatenated_data.to_csv("/content/concatenated_data.csv", index=False)

# Display the first few rows of the result
print(concatenated_data.head())